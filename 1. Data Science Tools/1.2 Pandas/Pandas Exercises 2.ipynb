{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7956232-61b8-4444-a192-ffa6a06a0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659cb29-d1cb-41ee-ac48-affa440abda1",
   "metadata": {},
   "source": [
    "# EX_1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cad7b775-9f5f-4d78-89b9-d1ffaaf1bc34",
   "metadata": {},
   "source": [
    "Create a series of 10 elements, random integers from 70 to 100, representing scores on a monthly exam. Set the index to be the month names, starting in September and ending in June. (If these months don’t match the school year in your location, feel free to make them more realistic.)\n",
    "With this series, write code to answer the following questions:\n",
    "- What is the student’s average test score for the entire year?\n",
    "- What is the student’s average test score during the first half of the year (i.e., the first five months)?\n",
    "- What is the student’s average test score during the second half of the year?\n",
    "- Did the student improve their performance in the second half? If so, by how much?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71337106-ec8b-42e1-b43f-f757a69ab96d",
   "metadata": {},
   "source": [
    "# EX_2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1858c95a-2bde-462f-9959-f092bfbff411",
   "metadata": {},
   "source": [
    "In high school, instructors sometimes gave extremely hard tests. Rather than fail most of the class, they would scale the test scores, known in some places as grading on a curve. They would assume that the average test score should be 80, calculate the difference between the actual mean and 80, and then add that difference to each of our scores.\n",
    "Generate 10 test scores between 40 and 60, again using an index starting with September and ending with June. \n",
    "Find the mean of the scores and add the difference between the mean and 80 to each of the scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dab5e0c-49d4-45b0-bd50-9864f8ba927d",
   "metadata": {},
   "source": [
    "# EX_3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9346a5e4-0364-4877-bbab-144a84d22233",
   "metadata": {},
   "source": [
    "Ggenerate 10 random integers in the range 0 to 100. (Remember that the np.random.randint function returns numbers that include the lower bound but exclude the upper bound.) Create a series containing those numbers’ tens digits. Thus, if our series contains 10, 25, 32, we want the series 1, 2, 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673c2ad-5062-4edb-a4c7-a1103ca80d08",
   "metadata": {},
   "source": [
    "# EX_4"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ee64550-d399-4460-ae0c-42f15c4326c7",
   "metadata": {},
   "source": [
    "The mean, median, and standard deviation are three numbers we can use to get a better picture of our data. Adding a few other numbers can give us an even more complete picture. These descriptive statistics typically include the mean, standard deviation, minimum value, 25% quantile, median, 50% quantile, and maximum value.\n",
    "Understanding and using descriptive statistics is a key skill for anyone working with data, and in this exercise, you’ll practice doing so with the following:\n",
    "- Generate a series of 100,000 floats in a normal distribution with a mean of 0 and a standard deviation of 100.\n",
    "- Get the descriptive statistics for this series. How close are the mean and median? (You don’t need to calculate the difference; rather, consider why they aren’t the same.)\n",
    "- Replace the minimum value with 5 times the maximum value.\n",
    "- Get the descriptive statistics again. Did the mean, median, and standard deviations change from their previous values? (Again, it’s enough to see the difference without calculating it.) If so, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb119d6-3cf1-4e52-b6f0-ee1b6a847ad0",
   "metadata": {},
   "source": [
    "# EX_5"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb871e2f-7ad5-4a26-a5d5-58f568560513",
   "metadata": {},
   "source": [
    "Create a series of 28 temperature readings in Celsius, representing four seven-day weeks, randomly selected from a normal distribution with a mean of 20 and a standard deviation of 5, rounded to the nearest integer.\n",
    "The index should start with Sun, continue through Sat, and repeat Sun through Sat until each temperature has a value. The question is, what was the mean temperature on Mondays during this period?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be677f-2f17-41e5-9f09-c06a4cecaab7",
   "metadata": {},
   "source": [
    "# EX_6"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bcba955-b384-4d5b-8208-c4dfe90b0147",
   "metadata": {},
   "source": [
    "The file 'data/taxi-passenger-count.csv' shows the number of passengers in each of 100,000 taxi rides.\n",
    "Show what percentage of taxi rides had only one passenger versus the (theoretical) maximum of six passengers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59adf60a-1eba-4863-8bba-ab58003b7d9c",
   "metadata": {},
   "source": [
    "# EX_7"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a9dfeaf-3738-46aa-8859-1808e5b31304",
   "metadata": {},
   "source": [
    "Load 'data/taxi-distance.csv' into a series. Then modify the series (or create another series) containing category names rather than numbers based on these criteria:\n",
    "- Short, ≤ 2 miles\n",
    "- Medium, > 2 miles but ≤ 10 miles\n",
    "- Long, > 10 miles\n",
    "Calculate the number of rides in each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49d30c-66f3-470d-b739-e18453b4eebd",
   "metadata": {},
   "source": [
    "# DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb596086-7919-40c1-9191-e6c66098a466",
   "metadata": {},
   "source": [
    "# EX_8"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2afeed10-528e-44f6-b53b-8145c160417e",
   "metadata": {},
   "source": [
    "Create a data frame that represents a company’s inventory of five products. Each product has a unique ID number (a two-digit integer will do), name, wholesale price, retail price, and number of sales in the last month.\n",
    "Once you have created this data frame, calculate the total net revenue from all your products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4bec0f-3b6e-42b2-aded-f284ebe26587",
   "metadata": {},
   "source": [
    "# EX_9"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c29b489-986d-4c06-9961-831e1ec84aae",
   "metadata": {},
   "source": [
    "Extend the previous data frame by adding columns.\n",
    "Your local government is thinking about imposing a sales tax and is considering 15%, 20%, and 25% rates. Show how much less you would net with each of these tax amounts by adding columns to the data frame for your net income under each of the proposed rates, as well as your current net income. (df.assign)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fee241-b724-4012-a8d5-d72715e52c50",
   "metadata": {},
   "source": [
    "# EX_10"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85ca4c3b-174b-40f0-9fd7-fcd1cb2a42bd",
   "metadata": {},
   "source": [
    "Your store is making money, and you have decided to add some new products.\n",
    "Create a new data frame and add it to the existing one. This new data frame should contain three products (including product ID,name, wholesale price, and retail price):\n",
    "\n",
    "- Phone, with an ID of 24, a wholesale price of 200, and a retail price of 500\n",
    "- Apple, with an ID of 16, a wholesale price of 0.5, and a retail price of 1\n",
    "- Pear, with an ID of 17, a wholesale price of 0.6, and a retail price of 1.2\n",
    "\n",
    "Because these are new products, don’t include the sales column. And to avoid problems and conflicts, ensure that the indexes of these new products are different from existing product indexes.\n",
    "\n",
    "Once you have added these new products, assign sales figures to each of them.\n",
    "Finally, recalculate the store’s total net income, including the new products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ebc760-6b97-4012-a119-f4fef677c125",
   "metadata": {},
   "source": [
    "# EX_11"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fa29671-dad3-4b61-8208-7ac581ac6738",
   "metadata": {},
   "source": [
    "Find the IDs and names of products that have sold more than the average number of units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c099842b-dea7-4584-8f4b-f8223b98e787",
   "metadata": {},
   "source": [
    "# EX_12"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcd28753-146f-47d9-8d24-6b51cfc207ca",
   "metadata": {},
   "source": [
    "Create a two-column data frame from the taxi data from 'data/taxi-passenger-count.csv' and 'data/taxi-distance.csv'. The first column will contain the passenger count for each trip, and the second column will contain the distance (in miles) for each trip. Once you have created this data frame: \n",
    "\n",
    "- Count how many trip distances were outliers.\n",
    "- Calculate the mean number of passengers for outliers. Is it different from the mean number of passengers for all trips?\n",
    "\n",
    "NOTE: The term outliers doesn’t have a precise, standard definition. Many people define it using the interquartile range (IQR), which is the value at the 75% point (aka quantile(0.75)) minus the value at the 25% point (aka quantile(0.25)). Outliers are then values below the 25% point –1.5 * IQR or values above the 75% + 1.5 * IQR. We use that definition here, but you may find that a different definition is a better fit for your data (say, anything below the mean - two standard deviations, or above the mean + two standard deviations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70db91-33f4-4b32-98c3-0132aa7adea3",
   "metadata": {},
   "source": [
    "# EX_13"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7266a96-b17c-461e-9528-8cc9837dcddf",
   "metadata": {},
   "source": [
    "In this exercise, we load some basic temperature data from New York City from the end of 2018 and the start of 2019. We then simulate a simple recurring equipment failure at 3:00 and 6:00 a.m. preventing us from getting temperature readings at those hours. How well does interpolation help us, and how far off are the interpolated mean and median calculations from the original, true values?\n",
    "\n",
    "1) Load the temperature data from New York City ('data/nyc-temps.txt') into a series. The measurements are in degrees Celsius.\n",
    "2) Create a data frame with two columns: temp, with the temperatures, and hour, representing the hours at which the measurements were taken. The hour values should be 0, 3, 6, 9, 12, 15, 18, and 21, repeated for all 728 data points.\n",
    "3) Calculate the mean and median values. These are the real values, which we hope to replicate via interpolation.\n",
    "4) Set all values from 3:00 and 6:00 a.m. to NaN.\n",
    "5) Interpolate the values with the 'interpolate' method.\n",
    "6) What are the mean and median of the interpolated data frame? Are they similar to the real values? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b13196e-e07d-4374-bd61-f37b049900cf",
   "metadata": {},
   "source": [
    "# EX_14"
   ]
  },
  {
   "cell_type": "raw",
   "id": "720431e9-ac18-4e93-bf21-43d9cf433220",
   "metadata": {},
   "source": [
    "Create the same two-column data frame as in the last exercise. Then, update the values in the temperature column so that any value less than 0 is set to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba47163c-7cc7-4a71-89f6-e79680e47304",
   "metadata": {},
   "source": [
    "# EX_15"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbb29d74-9867-4fdd-aa35-2e8b60a623f6",
   "metadata": {},
   "source": [
    "We have a database of parking tickets given in New York City in 2020. Let’s consider that this data was entered by a police officer, a parking inspector, or another person, which means there is a good chance it sometimes has missing or incorrect data. That may seem like a minor problem, but it can mean everything from cars being ticketed incorrectly to bad statistics in the system to people getting out of fines due to incorrect information.\n",
    "In this exercise, we will identify missing values, one of the most common problems you will encounter. We’ll see how often values are missing and what effect they may have. For this exercise, we’re going to assume that a parking ticket that is missing data may be dismissed.\n",
    "\n",
    "1. Create a data frame from the file 'data/nyc-parking-violations-2020_cut'.csv. We are only interested in the columns:\n",
    "    – Plate ID\n",
    "    – Registration State\n",
    "    – Vehicle Make\n",
    "    – Vehicle Color\n",
    "    – Violation Time\n",
    "    – Street Name\n",
    "How many rows are in the data frame when it is read into memory?\n",
    "\n",
    "2. Remove rows with any missing data (i.e., a NaN value). How many rows remain after doing this pruning? If each parking ticket brings $100 into the city, and missing data means the ticket can be successfully contested, how much money may New York City lose due to such missing data?\n",
    "\n",
    "3. Let’s instead assume that a ticket can only be dismissed if the license plate, state, car make, and/or street name are missing. Remove rows that are missing one or more of these. How many rows remain? Assuming $100/ticket, how much money would the city lose as a result of this missing data?\n",
    "\n",
    "4. Now let’s assume that tickets can be dismissed if the license plate, state, and/or street name are missing—that is, the same as the previous question, but without requiring the make of car. Remove rows that are missing one or more of these.\n",
    "How many rows remain? Assuming $100/ticket, how much money would the city lose as a result of this missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cf8b9e-3fd9-4c12-92d5-1b667cf6baaa",
   "metadata": {},
   "source": [
    "# EX_16"
   ]
  },
  {
   "cell_type": "raw",
   "id": "315e9b6b-d9e4-4078-bb09-68d52146e42a",
   "metadata": {},
   "source": [
    "Sometimes, a small fraction of the data is unreadable, missing, or corrupt. In other cases, a much larger proportion is problematic—and if you want to use the data set, you’ll need to not only remove bad data but also salvage the good data.\n",
    "For this exercise, we’ll look at a (slightly morbid) data set: a list of celebrities who died in 2016 and whose passing was recorded in Wikipedia, including the date of death, a short biography, and the cause of death. The problem is that this data set is messy, with some missing data and some erroneous data that will prevent us from working with it easily.\n",
    "\n",
    "The goal of this exercise is to find the average age of celebrities who died February–July 2016. Getting there will take several steps:\n",
    "\n",
    "1. Create a data frame from the file 'data/celebrity_deaths_2016.csv'. For this exercise, we’ll use only two columns:\n",
    "    – dateofdeath\n",
    "    – age\n",
    "2. Create a new month column containing the month from the dateofdeath column. (Use Pandas' 'str' accessor).\n",
    "3. Make the month column the index of the data frame (use pd.set_index).\n",
    "4. Sort the data frame by the index (sort_index()).\n",
    "5. Clean all nonintegers from the age column (astype()).\n",
    "6. Turn the age column into an integer value.\n",
    "7. Find the average age of celebrities who died during that period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e7f3d-d2b0-4e78-b0db-d20c31f37ade",
   "metadata": {},
   "source": [
    "# EX_17"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbc778ae-8ccf-4f89-8461-b4268c6f422c",
   "metadata": {},
   "source": [
    "Missing data is a common problem you must deal with when importing data sets. But equally common is inconsistent data, when the same value is represented by several different values, for example:\n",
    "\n",
    "    - United States of America\n",
    "    - USA\n",
    "    - U.S.A.\n",
    "    - U.S.A\n",
    "    - United States\n",
    "    - US\n",
    "    - U.S.\n",
    "\n",
    "Although people understand that these refer to the same country, a computer doesn’t. If your data is inconsistent, it will be hard for you to analyze it in any sort of serious way. Thus, a big part of cleaning real-world data involves making it more consistent.\n",
    "\n",
    "In this exercise, we return to the parking tickets database, trying to make it more consistent and thus easier to analyze. \n",
    "\n",
    "1. Create a data frame from the file 'data/nyc-parking-violations-2020_cut.csv'. We are only interested in the columns:\n",
    "    – Plate ID\n",
    "    – Registration State\n",
    "    – Vehicle Make\n",
    "    – Vehicle Color\n",
    "    – Street Name\n",
    "\n",
    "2. Determine how many different vehicle colors (the Vehicle Color column) there are.\n",
    "\n",
    "3. Look at the 30 most common colors, and identify colors that appear multiple times but are written differently. For example, the color WHITE is also written WT, WT., and WHT.\n",
    "\n",
    "4. Prepare a Python dict in which the keys represent the various color-name inputs and the values represent the values you want them to have in the end. I suggest using longer names, such as WHITE, rather than shorter ones.\n",
    "\n",
    "5. Replace the existing (old) colors with your translations. How many colors are there now?\n",
    "\n",
    "6. Look through the top 50 colors now that you have removed a bunch of them. Are there any you could still clean up? Are there any you cannot figure out? Can you identify some consistent typos and errors in the colors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dc380b-b9b9-4a03-af5b-b35866970597",
   "metadata": {},
   "source": [
    "# EX_18"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3cf63b7-26b3-4c76-851f-61dfeb762820",
   "metadata": {},
   "source": [
    "1. Load the CSV file 'data/nyc_taxi_2019-01_cut.csv' into a data frame using only the columns passenger_count, trip_distance, and total_amount.\n",
    "\n",
    "2. Using a descending sort, find the average cost of the 20 longest (in distance) taxi rides in January 2019.\n",
    "\n",
    "3. Using an ascending sort, find the average cost of the 20 longest (in distance) taxi rides in January 2019. Are the results any different?\n",
    "\n",
    "4. Sort by ascending passenger count and descending trip distance. (So, start with the longest trip with 0 passengers and end with the shortest trip with 9 passengers.)\n",
    "\n",
    "What is the average price paid for the top 50 rides?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de0f34-d354-400b-aece-bef1143adc6f",
   "metadata": {},
   "source": [
    "# EX_19"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3cc447c1-20b0-4946-983c-113715bb70be",
   "metadata": {},
   "source": [
    "1 Load taxi data from January 2019 into a data frame using only the columns passenger_count, trip_distance, and total_amount.\n",
    "\n",
    "2 For each number of passengers, find the mean cost of a taxi ride. Sort this result from lowest (i.e., cheapest) to highest (i.e., most expensive). (Use groupby()).\n",
    "\n",
    "3 Sort the results again by increasing the number of passengers.\n",
    "\n",
    "4 Create a new column, trip_distance_group, in which the values are short (< 2 miles), medium (≥ 2 miles and ≤ 10 miles), and long (> 10 miles). What is the average number of passengers per trip length category? Sort this result from highest (most passengers) to lowest (fewest passengers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c9d60-025c-487f-9176-240700b75e12",
   "metadata": {},
   "source": [
    "# EX_20"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9ab7e8a-fa01-49f6-ba61-5d1d5b3b048a",
   "metadata": {},
   "source": [
    "1. Take the eight CSV files provided in the 'data/temperature' folder, containing weather data from eight different cities (spanning four states), and turn them into a data frame. The files are 'san+francisco,ca.csv', 'new+york,ny.csv', 'springfield,ma.csv', 'boston,ma.csv', 'springfield,il.csv', 'albany,ny.csv', 'los+angeles,ca.csv', and 'chicago,il.csv'.\n",
    "\n",
    "2. We are only interested in the first three columns from each CSV file: the date and time, the max temperature, and the min temperature.\n",
    "\n",
    "3. Add city and state columns that contain the city and state from the filename and allow us to distinguish between rows.\n",
    "\n",
    "Once you’ve done all that, answer the following questions:\n",
    "\n",
    "- Does the data for each city and state start and end at (roughly) the same time? How do you know?\n",
    "- What is the lowest minimum temperature recorded for each city in the data set?\n",
    "- What is the highest maximum temperature recorded in each state in the data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc08c915-07e3-463b-a998-5288cc9dca37",
   "metadata": {},
   "source": [
    "# EX_21"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c0972b6-9e3d-45f1-9115-7967c282fd53",
   "metadata": {},
   "source": [
    "The calculations we’ll make in this exercise all take advantage of the filter and transform methods on DataFrameGroupBy objects. These methods allow us to conditionally keep (filter) and modify (transform) rows in a data frame while having access to all rows of the group when deciding and calculating.\n",
    "\n",
    "1. Read in the data frames for the city weather as in exercise 20, reading three columns: max_temp, min_temp, and precipMM.\n",
    "\n",
    "2. Determine which cities had, on at least three occasions, precipitation of 15 mm or more.\n",
    "\n",
    "3. Find cities that had at least three measurements of 10 mm of precipitation or more when the temperature was at or below 0° Celsius.\n",
    "\n",
    "4. For each precipitation measurement, calculate the proportion of that city’s total precipitation.\n",
    "\n",
    "5. For each city, determine the greatest proportion of that city’s total precipitation to fall in a given period."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
